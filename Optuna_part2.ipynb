{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5091870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import compose, pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import compose, pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, r2_score\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna import create_study\n",
    "from sklearn import compose\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6719683",
   "metadata": {},
   "source": [
    "### Search function for each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9028ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_ridge(trial : Trial) -> Ridge:\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", .1, 100, log=True),\n",
    "    }\n",
    "\n",
    "    return Ridge(**params)\n",
    "\n",
    "def instantiate_lasso(trial : Trial) -> Lasso:\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1, log=True),\n",
    "    }\n",
    "\n",
    "    return Lasso(**params)\n",
    "\n",
    "def instantiate_xgb(trial : Trial) -> XGBRegressor:\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"n_estimators\": 1000,\n",
    "        \"verbosity\": 0,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "    }\n",
    "\n",
    "    return XGBRegressor(**params)\n",
    "\n",
    "Classifier = (\n",
    "    Ridge |\n",
    "    Lasso |\n",
    "    XGBRegressor )\n",
    "\n",
    "def instantiate_learner(trial : Trial) -> Classifier:\n",
    "    algorithm = trial.suggest_categorical(\n",
    "    'algorithm', ['ridge', 'lasso', 'xgb'])\n",
    "    \n",
    "    if algorithm =='ridge':\n",
    "        model = instantiate_ridge(trial)\n",
    "    elif algorithm=='lasso':\n",
    "        model = instantiate_lasso(trial)\n",
    "    elif algorithm=='xgb':\n",
    "        model = instantiate_xgb(trial)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "# from category_encoders import WOEEncoder\n",
    "\n",
    "# Encoder = (\n",
    "#   OrdinalEncoder |\n",
    "#   OneHotEncoder \n",
    "# )\n",
    "\n",
    "# def instantiate_encoder(trial : Trial) -> Encoder:\n",
    "#     method = trial.suggest_categorical(\n",
    "#     'encoding_method', ['ordinal', 'onehot']\n",
    "#   )\n",
    "#     if method=='ordinal':\n",
    "#     encoder = instantiate_ordinal_encoder(trial)\n",
    "#     elif method=='onehot':\n",
    "#     encoder = instantiate_onehot_encoder(trial)\n",
    "    \n",
    "#     return encoder\n",
    "\n",
    "# from sklearn.preprocessing import (\n",
    "#   StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb13d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95f7cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# def instantiate_numerical_pipeline(trial : Trial) -> Pipeline:\n",
    "#     pipeline = Pipeline([\n",
    "#     ('scaler', instantiate_scaler(trial))\n",
    "#   ])\n",
    "#     return pipeline\n",
    "\n",
    "# def instantiate_categorical_function(trial : Trial) -> Pipeline:\n",
    "#     pipeline = Pipeline([\n",
    "#     ('encoder', instantiate_encoder(trial))\n",
    "#   ])\n",
    "#     return pipeline\n",
    "\n",
    "def instantiate_processor(trial : Trial, \n",
    "                          numerical_columns : list[str], \n",
    "                          categorical_columns : list[str]) -> ColumnTransformer:\n",
    "    \n",
    "    numerical_pipeline = StandardScaler()\n",
    "    categorical_pipeline = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    \n",
    "#     numerical_pipeline = instantiate_numerical_pipeline(trial)\n",
    "#     categorical_pipeline = instantiate_categorical_pipeline(trial)\n",
    "    \n",
    "    processor = ColumnTransformer([\n",
    "        ('numerical_pipeline', numerical_pipeline, numerical_columns),\n",
    "        ('categorical_pipeline', categorical_pipeline, categorical_columns)\n",
    "    ])\n",
    "    return processor\n",
    "\n",
    "def instantiate_model(trial : Trial, numerical_columns : list[str], \n",
    "                      categorical_columns : list[str]) -> Pipeline:\n",
    "    \n",
    "    processor = instantiate_processor(\n",
    "        trial, numerical_columns, categorical_columns\n",
    "    )\n",
    "    \n",
    "    learner = instantiate_learner(trial)\n",
    "    \n",
    "    model_pipe = Pipeline([\n",
    "    ('processor', processor),\n",
    "    ('model', learner)\n",
    "    ])\n",
    "    \n",
    "    model = compose.TransformedTargetRegressor(regressor= model_pipe,\n",
    "                                                func=np.log, inverse_func=np.exp)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "810cda1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial : Trial, X : DataFrame,\n",
    "              y : np.ndarray | Series, \n",
    "              numerical_columns : Optional[list[str]]=None, \n",
    "              categorical_columns : Optional[list[str]]=None, \n",
    "              random_state : int=42) -> float:\n",
    "    \n",
    "    if numerical_columns is None:\n",
    "        numerical_columns = [\n",
    "            *X.select_dtypes(exclude=['object', 'category']).columns\n",
    "        ]\n",
    "    \n",
    "    if categorical_columns is None:\n",
    "        categorical_columns = [\n",
    "            *X.select_dtypes(include=['object', 'category']).columns\n",
    "        ]\n",
    "    \n",
    "    model = instantiate_model(trial, numerical_columns, categorical_columns)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    r2 = make_scorer(r2_score)\n",
    "    scores = cross_val_score(model, X, y, scoring= r2, cv=kf)\n",
    "    \n",
    "    return np.min([np.mean(scores), np.median([scores])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c888d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_normal_quality.csv', index_col=0)\n",
    "y = df.SalePrice\n",
    "X = df.drop(['PID', 'SalePrice'], axis =1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c10a9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-06 13:39:12,914] A new study created in memory with name: optimization\n"
     ]
    }
   ],
   "source": [
    "from optuna import create_study\n",
    "\n",
    "study = create_study(study_name='optimization', direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff1be73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-06 13:39:22,576] Trial 0 finished with value: 0.9493219060468837 and parameters: {'algorithm': 'ridge', 'alpha': 7.865444908605931}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:39:34,759] Trial 1 finished with value: -6.469745399788328 and parameters: {'algorithm': 'xgb', 'learning_rate': 0.0010113605272903524, 'max_depth': 5, 'subsample': 0.8700579565766164, 'colsample_bytree': 0.8467118630224082, 'min_child_weight': 14}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:16,373] Trial 2 finished with value: 0.9466220933014494 and parameters: {'algorithm': 'xgb', 'learning_rate': 0.031692312425249475, 'max_depth': 8, 'subsample': 0.7062348936986825, 'colsample_bytree': 0.9797195042020221, 'min_child_weight': 16}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:16,742] Trial 3 finished with value: 0.9489470072119932 and parameters: {'algorithm': 'ridge', 'alpha': 4.8171047989234275}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:17,164] Trial 4 finished with value: 0.9427585952976025 and parameters: {'algorithm': 'lasso', 'alpha': 0.0026021458855876494}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:17,579] Trial 5 finished with value: 0.9488292062341029 and parameters: {'algorithm': 'ridge', 'alpha': 4.194601121043548}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:18,001] Trial 6 finished with value: 0.9487888821590815 and parameters: {'algorithm': 'ridge', 'alpha': 4.00419812164468}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:18,425] Trial 7 finished with value: 0.9478883258206032 and parameters: {'algorithm': 'ridge', 'alpha': 1.4632072078782992}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:18,847] Trial 8 finished with value: 0.9390431778895205 and parameters: {'algorithm': 'lasso', 'alpha': 0.006070677526234753}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:19,274] Trial 9 finished with value: 0.9491518389083822 and parameters: {'algorithm': 'ridge', 'alpha': 54.54503818794944}. Best is trial 0 with value: 0.9493219060468837.\n",
      "[I 2024-05-06 13:40:19,928] Trial 10 finished with value: 0.950206827235748 and parameters: {'algorithm': 'lasso', 'alpha': 0.0003786753353101835}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:20,944] Trial 11 finished with value: 0.9497470055568982 and parameters: {'algorithm': 'lasso', 'alpha': 0.0001072723986448871}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:21,991] Trial 12 finished with value: 0.9498189759502026 and parameters: {'algorithm': 'lasso', 'alpha': 0.00011476379151174207}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:23,010] Trial 13 finished with value: 0.949826248228858 and parameters: {'algorithm': 'lasso', 'alpha': 0.00011572123571151853}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:23,554] Trial 14 finished with value: 0.9491875349922563 and parameters: {'algorithm': 'lasso', 'alpha': 0.0007261763874637471}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:24,141] Trial 15 finished with value: 0.9498065738684014 and parameters: {'algorithm': 'lasso', 'alpha': 0.000543418298988824}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:24,643] Trial 16 finished with value: 0.9112482774206228 and parameters: {'algorithm': 'lasso', 'alpha': 0.0265610722457296}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:25,210] Trial 17 finished with value: 0.9500507550695927 and parameters: {'algorithm': 'lasso', 'alpha': 0.00045319973265653537}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:32,782] Trial 18 finished with value: -0.8614947521323127 and parameters: {'algorithm': 'xgb', 'learning_rate': 0.003171134764471012, 'max_depth': 1, 'subsample': 0.07607380815411469, 'colsample_bytree': 0.10153081382378465, 'min_child_weight': 1}. Best is trial 10 with value: 0.950206827235748.\n",
      "[I 2024-05-06 13:40:33,233] Trial 19 finished with value: 0.9021850084069876 and parameters: {'algorithm': 'lasso', 'alpha': 0.031223964494168983}. Best is trial 10 with value: 0.950206827235748.\n"
     ]
    }
   ],
   "source": [
    "study.optimize(lambda trial: objective(trial, X, y), n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a0e8949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=10, state=1, values=[0.950206827235748], datetime_start=datetime.datetime(2024, 5, 6, 13, 40, 19, 276115), datetime_complete=datetime.datetime(2024, 5, 6, 13, 40, 19, 928448), params={'algorithm': 'lasso', 'alpha': 0.0003786753353101835}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'algorithm': CategoricalDistribution(choices=('ridge', 'lasso', 'xgb')), 'alpha': FloatDistribution(high=1.0, log=True, low=0.0001, step=None)}, trial_id=10, value=None)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# study without drop and 20 trials \n",
    "study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7da5ce05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FrozenTrial(number=2, state=1, values=[0.9499955354069816], datetime_start=datetime.datetime(2024, 5, 6, 13, 32, 15, 632163), datetime_complete=datetime.datetime(2024, 5, 6, 13, 32, 16, 348714), params={'algorithm': 'lasso', 'alpha': 0.00022003193991056288}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'algorithm': CategoricalDistribution(choices=('ridge', 'lasso', 'xgb')), 'alpha': FloatDistribution(high=1.0, log=True, low=0.0001, step=None)}, trial_id=2, value=None)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# study with drop and 20 trials and target variable transformed \n",
    "study.best_trials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
